{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Source vector is calculated separately and implemented in loss"
      ],
      "metadata": {
        "id": "H4Na2XdENuch"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3H-Zd0rTPJF",
        "outputId": "a5fc1d08-b014-48b5-fe04-115768b8b4a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyDOE\n",
            "  Downloading pyDOE-0.3.8.zip (22 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyDOE) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyDOE) (1.10.1)\n",
            "Building wheels for collected packages: pyDOE\n",
            "  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyDOE: filename=pyDOE-0.3.8-py3-none-any.whl size=18168 sha256=09144dc7b90feffd8c78e7db78bd5408da56affb6933594f19758c803fb10be6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/b6/d7/c6b64746dba6433c593e471e0ac3acf4f36040456d1d160d17\n",
            "Successfully built pyDOE\n",
            "Installing collected packages: pyDOE\n",
            "Successfully installed pyDOE-0.3.8\n"
          ]
        }
      ],
      "source": [
        "!pip install pyDOE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "from pyDOE import lhs\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from matplotlib.ticker import LinearLocator\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as mpl\n",
        "mpl.rcParams.update(mpl.rcParamsDefault)\n",
        "from scipy.interpolate import griddata\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "# generates same random numbers each time\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)\n",
        "\n",
        "print(\"TensorFlow version: {}\".format(tf.__version__))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu5O2EPiTjss",
        "outputId": "a58013d2-87ac-44d2-c0a7-2a1e648b3ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\" Original physical parameters \"\"\"\"\"\"\"\"\"\"\"\n",
        "# External source sineburst excitation parameters\n",
        "original_Lx = original_Ly = 0.3 # x-length and y-length of the steel plate[m]\n",
        "original_c = 4773.343 # [m/s]  wave propagation speed in steel plate, v = sqrt(E/rho)\n",
        "original_rho = 7900 # [kg/m^3] density of steel plate\n",
        "original_fex = 120000 # frequency in Hz used in excitation\n",
        "original_Tex = 1/original_fex\n",
        "cycles = 5\n",
        "original_dt = original_Tex /40  # in seconds\n",
        "original_tsim = 1.2*original_Lx/2/original_c; # total simulation time\n",
        "print('original_dt : ', original_dt, ' original_tsim : ', original_tsim)\n",
        "original_Vamp = 10e-9;\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\" Scaled parameters between [0, 1] \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "scaled_Lx = scaled_Ly = 1.0 # scaled x-length and y-length of the steel plate\n",
        "scaling_length_factor = original_Lx*scaled_Lx\n",
        "beta = scaling_length_factor\n",
        "print('Scaled length parameter (beta) : ', beta)\n",
        "scaled_tsim = 1.0\n",
        "scaling_time_factor = original_tsim*scaled_tsim\n",
        "alpha = scaling_time_factor\n",
        "print('Scaled time parameters (alpha) : ', alpha)\n",
        "scaled_c = original_c*(alpha/beta)\n",
        "print('Scaled velocity: ', scaled_c)\n",
        "scaled_fex = original_fex * alpha;\n",
        "scaled_Tex = 1/scaled_fex; # scaled time period\n",
        "scaled_dt = scaled_Tex * scaled_tsim/40;\n",
        "scaled_Vamp = original_Vamp*scaling_length_factor\n",
        "print('scaled_dt : ', scaled_dt, ' scaled_tsim : ', scaled_tsim)\n",
        "\n",
        "lb = np.array([0.0, 0.0, 0.0,0.0,0.0]) # (x_0,y_0,t_0)\n",
        "\n",
        "ub =  np.array([1.0, 1.0, 1.0, 1.0, 1.0])  # (x_n,y_n,t_n)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGlDtk6gTlL1",
        "outputId": "8c4b9afb-8772-4d81-ae5e-6988a97d6020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original_dt :  2.0833333333333333e-07  original_tsim :  3.770942083986003e-05\n",
            "Scaled length parameter (beta) :  0.3\n",
            "Scaled time parameters (alpha) :  3.770942083986003e-05\n",
            "Scaled velocity:  0.6\n",
            "scaled_dt :  0.005524702546296296  scaled_tsim :  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0.8*scaled_Tex*5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjiLDt9IPkyv",
        "outputId": "e69f2e1d-ce2b-499f-ac8d-d2dfc1806783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8839524074074073"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\" Imported scaled data from MATLAB \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "# Load Data\n",
        "data = scipy.io.loadmat('wave2d_damage_scaled.mat')\n",
        "disp_data = data['saved_dis'] # 151 x 151 x 182 # actual dimension\n",
        "scaled_disp_data = data['scaled_w'] # 151 x 151 x 182 # scaled dimension\n",
        "t_data = data['time_data'] # 1 x 182\n",
        "x_data = data['x'] # 1 x 151\n",
        "y_data = data['y'] # 1 x 151\n",
        "print('disp_data : ', np.shape(disp_data))\n",
        "print('scaled_disp_data : ', np.shape(scaled_disp_data))\n",
        "print('t_data : ', np.shape(t_data))\n",
        "print('x_data : ', np.shape(x_data))\n",
        "print('y_data : ', np.shape(y_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amRzTyd4r-sZ",
        "outputId": "99c609c8-e004-4499-d097-fab0bbb99191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "disp_data :  (151, 151, 182)\n",
            "scaled_disp_data :  (151, 151, 182)\n",
            "t_data :  (1, 182)\n",
            "x_data :  (1, 151)\n",
            "y_data :  (1, 151)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modifying shapes of input MATLAB data to be compatible for training\n",
        "\"\"\"\"\"\"\"\"\"\"\" Space co-ordinates \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "x_data = x_data.T # 151 x 1\n",
        "y_data = y_data.T # 151 x 1\n",
        "X,Y = np.meshgrid(x_data,y_data)\n",
        "X_Y_data = np.hstack((X.flatten()[:,None], Y.flatten()[:,None]))\n",
        "print('X_Y_data : ',np.shape(X_Y_data)) # 22801 x 2\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\" Displacement field (actual) \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "disp_data = disp_data.reshape(-1,182)\n",
        "print('disp_data : ',np.shape(disp_data))  # 22801 x 182\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\" Displacement field (scaled) \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "scaled_disp_data = scaled_disp_data.reshape(-1,182)\n",
        "print('scaled_disp_data : ',np.shape(scaled_disp_data))  # 22801 x 182\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Lets set equal shape for all input and output data\n",
        "TT= np.tile(t_data,(22801, 1)) # repeats time_data row-wise\n",
        "print('t_star : ',np.shape(TT))\n",
        "XX= np.tile(X_Y_data[:,0:1],(1,182)) # repeats x_star column-wise\n",
        "print('x_star : ',np.shape(XX))\n",
        "YY= np.tile(X_Y_data[:,1:2],(1,182)) # repeats y_star column-wise\n",
        "print('y_star : ',np.shape(YY))\n",
        "UU = disp_data\n",
        "print('u_star : ',np.shape(UU))\n",
        "UU_scaled = scaled_disp_data\n",
        "print('u_star_scaled : ',np.shape(UU_scaled))\n",
        "\n",
        "# Reshape input and outputs as column vector\n",
        "print('After reshaping to column vectors : ')\n",
        "t_star = np.reshape(TT,(-1,1))\n",
        "print('t_star : ',np.shape(t_star))\n",
        "x_star = np.reshape(XX,(-1,1))\n",
        "print('x_star : ',np.shape(x_star))\n",
        "y_star = np.reshape(YY,(-1,1))\n",
        "print('y_star : ',np.shape(y_star))\n",
        "u_star = np.reshape(UU,(-1,1))\n",
        "print('u_star : ',np.shape(u_star))\n",
        "u_star_scaled = np.reshape(UU_scaled,(-1,1))\n",
        "print('u_star_scaled : ',np.shape(u_star_scaled))\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\" Damage location parameters \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "damage_x = np.zeros((151,1))\n",
        "damage_x[0:151] = 0.4\n",
        "\n",
        "damage_y = np.zeros((151,1))\n",
        "damage_y[0:151] = 0.3\n",
        "\n",
        "Dx,Dy = np.meshgrid(damage_x,damage_y)\n",
        "Dx_Dy_data = np.hstack((Dx.flatten()[:,None], Dy.flatten()[:,None]))\n",
        "print('Dx_Dy_data : ',np.shape(Dx_Dy_data)) # 22801 x 2\n",
        "\n",
        "dx_star = np.tile(Dx_Dy_data [:,0:1],(1,182)) # repeats dx_star column-wise\n",
        "print('dx_star : ',np.shape(dx_star))\n",
        "dy_star = np.tile(Dx_Dy_data [:,1:2],(1,182)) # repeats dy_star column-wise\n",
        "print('dy_star : ',np.shape(dy_star))\n",
        "\n",
        "dx_star = np.reshape(dx_star,(-1,1))\n",
        "print('dx_star after reshaping : ',np.shape(dx_star))\n",
        "dy_star = np.reshape(dy_star,(-1,1))\n",
        "print('dy_star after reshaping : ',np.shape(dy_star))\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\" Damage size parameters \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "damage_sigma_x = np.zeros((151,1))\n",
        "damage_sigma_x[0:151] = 0.03\n",
        "\n",
        "damage_sigma_y = np.zeros((151,1))\n",
        "damage_sigma_y[0:151] = 0.03\n",
        "\n",
        "Dsigma_x, Dsigma_y = np.meshgrid(damage_sigma_x,damage_sigma_y)\n",
        "Dx_Dy_sigma_data = np.hstack((Dsigma_x.flatten()[:,None], Dsigma_y.flatten()[:,None]))\n",
        "print('Dx_Dy_sigma_data : ',np.shape(Dx_Dy_sigma_data)) # 22801 x 2\n",
        "\n",
        "d_sigma_x_star = np.tile(Dx_Dy_sigma_data [:,0:1],(1,182)) # repeats dx_star column-wise\n",
        "print('d_sigma_x_star : ',np.shape(d_sigma_x_star))\n",
        "d_sigma_y_star = np.tile(Dx_Dy_sigma_data [:,1:2],(1,182)) # repeats dy_star column-wise\n",
        "print('d_sigma_y_star : ',np.shape(d_sigma_y_star))\n",
        "\n",
        "d_sigma_x_star = np.reshape(d_sigma_x_star,(-1,1))\n",
        "print('d_sigma_x_star after reshaping : ',np.shape(d_sigma_x_star))\n",
        "d_sigma_y_star = np.reshape(d_sigma_y_star,(-1,1))\n",
        "print('d_sigma_y_star after reshaping : ',np.shape(d_sigma_y_star))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vuyPKnis63o",
        "outputId": "b8a3b956-d83e-43ff-b05d-c538fb7d7130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_Y_data :  (22801, 2)\n",
            "disp_data :  (22801, 182)\n",
            "scaled_disp_data :  (22801, 182)\n",
            "t_star :  (22801, 182)\n",
            "x_star :  (22801, 182)\n",
            "y_star :  (22801, 182)\n",
            "u_star :  (22801, 182)\n",
            "u_star_scaled :  (22801, 182)\n",
            "After reshaping to column vectors : \n",
            "t_star :  (4149782, 1)\n",
            "x_star :  (4149782, 1)\n",
            "y_star :  (4149782, 1)\n",
            "u_star :  (4149782, 1)\n",
            "u_star_scaled :  (4149782, 1)\n",
            "Dx_Dy_data :  (22801, 2)\n",
            "dx_star :  (22801, 182)\n",
            "dy_star :  (22801, 182)\n",
            "dx_star after reshaping :  (4149782, 1)\n",
            "dy_star after reshaping :  (4149782, 1)\n",
            "Dx_Dy_sigma_data :  (22801, 2)\n",
            "d_sigma_x_star :  (22801, 182)\n",
            "d_sigma_y_star :  (22801, 182)\n",
            "d_sigma_x_star after reshaping :  (4149782, 1)\n",
            "d_sigma_y_star after reshaping :  (4149782, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def trainingdata(N_u_ic, N_u_bc, N_pde):\n",
        "\n",
        "  # Initial condition\n",
        "  t_b0 = t_star[t_star==t_star.min()][:,None]\n",
        "  x_b0 = x_star[t_star == t_star.min()][:,None]\n",
        "  y_b0 = y_star[t_star == t_star.min()][:,None]\n",
        "  dx_b0 = dx_star[t_star == t_star.min()][:,None]\n",
        "  dy_b0 = dy_star[t_star == t_star.min()][:,None]\n",
        "  X_b0 = np.concatenate([x_b0,y_b0,t_b0,dx_b0,dy_b0],1)  # concatenate  x_b0, y_b0,t_b0, dx_b0, dy_b0\n",
        "  print('X_b0 : ', np.shape(X_b0))\n",
        "  u_b0 = u_star[t_star == t_star.min()][:,None]\n",
        "  print('t_b0 : ', np.shape(t_b0),'x_b0 : ', np.shape(x_b0),\n",
        "        'y_b0 : ', np.shape(y_b0),'dx_b0 : ', np.shape(dx_b0),\n",
        "        'dy_b0 : ', np.shape(dy_b0),'u_b0 : ', np.shape(u_b0) )\n",
        "\n",
        "\n",
        "  # Boundary condition - left side <--- 151 x 16 = (left boubary space grid points x timesteps)\n",
        "  t_b1 = t_star[x_star==x_star.min()][:,None]\n",
        "  x_b1 = x_star[x_star == x_star.min()][:,None]\n",
        "  y_b1 = y_star[x_star == x_star.min()][:,None]\n",
        "  dx_b1 = dx_star[x_star == x_star.min()][:,None]\n",
        "  dy_b1 = dy_star[x_star == x_star.min()][:,None]\n",
        "  X_b1 = np.concatenate([x_b1,y_b1,t_b1,dx_b1,dy_b1],1)  # concatenate x_b1, y_b1, t_b1, dx_b1, dy_b1\n",
        "  print('X_b1 : ', np.shape(X_b1))\n",
        "  u_b1 = u_star[x_star == x_star.min()][:,None]\n",
        "  print('t_b1 : ', np.shape(t_b1),'x_b1 : ', np.shape(x_b1),\n",
        "        'y_b1 : ', np.shape(y_b1),'dx_b1 : ', np.shape(dx_b1),\n",
        "        'dy_b1 : ', np.shape(dy_b1),'u_b1 : ', np.shape(u_b1))\n",
        "\n",
        "  # Boundary condition - right side <--- 151 x 16 = (right boubary space grid points x timesteps)\n",
        "  t_b2 = t_star[x_star==x_star.max()][:,None]\n",
        "  x_b2 = x_star[x_star == x_star.max()][:,None]\n",
        "  y_b2 = y_star[x_star == x_star.max()][:,None]\n",
        "  dx_b2 = dx_star[x_star == x_star.max()][:,None]\n",
        "  dy_b2 = dy_star[x_star == x_star.max()][:,None]\n",
        "  X_b2 = np.concatenate([x_b2,y_b2,t_b2,dx_b2,dy_b2],1)  # concatenate  x_b2, y_b2, t_b2, dx_b2, dy_b2\n",
        "  print('X_b2 : ', np.shape(X_b2))\n",
        "  u_b2 = u_star[x_star == x_star.max()][:,None]\n",
        "  print('t_b2 : ', np.shape(t_b2),'x_b2 : ', np.shape(x_b2),\n",
        "        'y_b2 : ', np.shape(y_b2),'dx_b2 : ', np.shape(dx_b2),\n",
        "        'dy_b2 : ', np.shape(dy_b2),'u_b2 : ', np.shape(u_b2))\n",
        "\n",
        "  # Boundary condition - bottom side <--- 151 x 16 = (bottom boubary space grid points x timesteps)\n",
        "  t_b3 = t_star[y_star==y_star.min()][:,None]\n",
        "  x_b3 = x_star[y_star == y_star.min()][:,None]\n",
        "  y_b3 = y_star[y_star == y_star.min()][:,None]\n",
        "  dx_b3 = dx_star[y_star == y_star.min()][:,None]\n",
        "  dy_b3 = dy_star[y_star == y_star.min()][:,None]\n",
        "  X_b3 = np.concatenate([x_b3,y_b3,t_b3,dx_b3,dy_b3],1)  # concatenate x_b3, y_b3, t_b3, dx_b3, dy_b3\n",
        "  print('X_b3 : ', np.shape(X_b3))\n",
        "  u_b3 = u_star[y_star == y_star.min()][:,None]\n",
        "  print('t_b3 : ', np.shape(t_b3),'x_b3 : ', np.shape(x_b3),\n",
        "        'y_b3 : ', np.shape(y_b3),'dx_b3 : ', np.shape(dx_b3),\n",
        "        'dy_b3 : ', np.shape(dy_b3),'u_b3 : ', np.shape(u_b3))\n",
        "\n",
        "  # Boundary condition - top side <--- 151 x 16 = (top boubary space grid points x timesteps)\n",
        "  t_b4 = t_star[y_star == y_star.max()][:,None]\n",
        "  x_b4 = x_star[y_star == y_star.max()][:,None]\n",
        "  y_b4 = y_star[y_star == y_star.max()][:,None]\n",
        "  dx_b4 = dx_star[y_star == y_star.max()][:,None]\n",
        "  dy_b4 = dy_star[y_star == y_star.max()][:,None]\n",
        "  X_b4 = np.concatenate([x_b4,y_b4,t_b4,dx_b4,dy_b4],1)  # concatenate x_b4, y_b4, t_b4, dx_b4, dy_b4\n",
        "  print('X_b4 : ', np.shape(X_b4))\n",
        "  u_b4 = u_star[y_star == y_star.max()][:,None]\n",
        "  print('t_b4 : ', np.shape(t_b4),'x_b4 : ', np.shape(x_b4),\n",
        "        'y_b4 : ', np.shape(y_b4),'dx_b4 : ', np.shape(dx_b4),\n",
        "        'dy_b4 : ', np.shape(dy_b4),'u_b4 : ', np.shape(u_b4))\n",
        "\n",
        "\n",
        "  #-----------------Initial condition data points-------------------------------\n",
        "  all_X_u_ic_train = np.vstack([X_b0])  # (,5) = (x,y,t,dx,dy)\n",
        "  print('all_X_u_ic_train : ', np.shape(all_X_u_ic_train))\n",
        "  all_u_ic_train = np.vstack([u_b0]) # (,1) = (u)\n",
        "  print('all_u_ic_train : ', np.shape(all_u_ic_train))\n",
        "\n",
        "  #choose random N_u_ic points for training for IC\n",
        "  idx = np.random.choice(all_X_u_ic_train.shape[0], N_u_ic, replace=False)\n",
        "\n",
        "  X_u_ic_train = all_X_u_ic_train[idx,:] # choose indices from set 'idx' (x,t)\n",
        "  print('X_u_ic_train from random sampling : ', np.shape(X_u_ic_train))\n",
        "  u_ic_train = all_u_ic_train[idx,:]\n",
        "  print('u_ic_train from random sampling : ', np.shape(u_ic_train))\n",
        "\n",
        "  #-------------------Boundary condition data points----------------------------\n",
        "\n",
        "  all_X_u_bc_train = np.vstack([X_b1, X_b2, X_b3, X_b4])  # (,5) = (x,y,t,dx,dy)\n",
        "  print('all_X_u_bc_train : ', np.shape(all_X_u_bc_train))\n",
        "  all_u_bc_train = np.vstack([u_b1, u_b2, u_b3, u_b4]) # (,1) = (u)\n",
        "  print('all_u_bc_train : ', np.shape(all_u_bc_train))\n",
        "\n",
        "  #choose random N_u_bc points for training at boundary points\n",
        "  idx = np.random.choice(all_X_u_bc_train.shape[0], N_u_bc, replace=False)\n",
        "  X_u_bc_train = all_X_u_bc_train[idx,:] # choose indices from set 'idx' (x,t)\n",
        "  print('X_u_bc_train from random sampling : ', np.shape(X_u_bc_train))\n",
        "  u_bc_train = all_u_bc_train[idx,:]\n",
        "  print('u_bc_train from random sampling : ', np.shape(u_bc_train))\n",
        "\n",
        "  #--------------------PDE data points------------------------------------------\n",
        "  # Random sampling for PDE points as well\n",
        "  idx = np.random.choice(t_star.shape[0], N_pde, replace=False)\n",
        "  t_pde_train = t_star[idx,:]\n",
        "  x_pde_train = x_star[idx,:]\n",
        "  y_pde_train = y_star[idx,:]\n",
        "  dx_pde_train = dx_star[idx,:]\n",
        "  dy_pde_train = dy_star[idx,:]\n",
        "  u_pde_train = u_star[idx,:]\n",
        "  print('t_pde_train : ',np.shape(t_pde_train), 'x_pde_train : ', np.shape(x_pde_train), 'y_pde_train : ',np.shape(y_pde_train),\n",
        "        'dx_pde_train : ',np.shape(dx_pde_train), 'dy_pde_train : ',np.shape(dy_pde_train),'u_pde_train : ',np.shape(u_pde_train))\n",
        "\n",
        "  X_pde_train = np.hstack([x_pde_train, y_pde_train, t_pde_train, dx_pde_train, dy_pde_train])\n",
        "  # adding source point - work from here\n",
        "  x_pde_train_source = np.ones((np.size(t_star),1))*scaled_Lx/2\n",
        "  y_pde_train_source =  np.ones((np.size(t_star),1))*scaled_Ly/2\n",
        "  t_pde_train_source = t_star#t_data.T\n",
        "  dx_pde_train_source = np.ones((np.size(t_star),1))*0.4\n",
        "  dy_pde_train_source = np.ones((np.size(t_star),1))*0.3\n",
        "  X_pde_train_source = np.hstack([x_pde_train_source, y_pde_train_source, t_pde_train_source, dx_pde_train_source, dy_pde_train_source])\n",
        "\n",
        "  #source_point = np.array([scaled_Lx/2, scaled_Ly/2, 0.0, 0.4, 0.3])\n",
        "  #X_pde_train = np.vstack([X_pde_train, X_pde_train_source])\n",
        "  print('X_f_train from random sampling : ', np.shape(X_pde_train))\n",
        "\n",
        "\n",
        "  return X_pde_train, X_pde_train_source, u_pde_train, X_u_ic_train, u_ic_train, X_u_bc_train, u_bc_train"
      ],
      "metadata": {
        "id": "0zYPyMzOVa7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(np.shape(t_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WszmUNnrYrne",
        "outputId": "cdf755b2-6cc1-415a-96c9-425d0c7b306d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 182)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_pde_train_source = np.ones((np.size(t_data),1))*scaled_Lx/2"
      ],
      "metadata": {
        "id": "n4QeUGinYmmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ParametricModel_Scaled(tf.Module):\n",
        "\n",
        "  def __init__(self, layers, name=None):\n",
        "    self.W = [] # weights and biases <----- initializing the array\n",
        "    self.parameters = 0 # total number of parameters <----- initializing the variable\n",
        "\n",
        "    for i in range(len(layers)-1):\n",
        "      input_dim = layers[i]\n",
        "      #print('input_dim : ', input_dim, 'in layer : ', i)\n",
        "      output_dim = layers[i+1]\n",
        "      #print('output_dim : ', output_dim, 'in layer : ', i)\n",
        "\n",
        "      # Xavier standard deviation = Glorot\n",
        "      std_dv = np.sqrt((2.0/(input_dim + output_dim)))\n",
        "      #print('std_dv : ', std_dv, 'in layer : ', i)\n",
        "\n",
        "      # weights = normal distribution*Xavier standard deviation + 0\n",
        "      w = tf.random.normal([input_dim, output_dim], dtype='float64')*std_dv\n",
        "      #print('weights with std_dv: ', w, np.shape(w))\n",
        "\n",
        "      w = tf.Variable(w,trainable=True,name='w'+str(i+1))\n",
        "      #print('weights as trainable variables: ', w, np.shape(w))\n",
        "\n",
        "      b = tf.Variable(tf.cast(tf.zeros([output_dim]),dtype='float64'), trainable=True,name='b'+str(i+1))\n",
        "      #print('biases : ',b, 'in layer : ', i)\n",
        "\n",
        "      self.W.append(w)\n",
        "      #print('W appends weights :',self.W,' in layer : ',i)\n",
        "      self.W.append(b)\n",
        "      #print('W appends biases :',self.W,' in layer : ',i)\n",
        "\n",
        "      self.parameters += input_dim*output_dim + output_dim\n",
        "      #print('No of parameters : ', self.parameters)\n",
        "\n",
        "  def get_weights(self):\n",
        "    parameters_1d = [] # [.....W_i,b_i,.......]\n",
        "    for i in range(len(layers)-1):\n",
        "      w_1d = tf.reshape(self.W[2*i],[-1]) # flatten weights\n",
        "      #print('w_1d : ', w_1d, 'in layer : ',i)\n",
        "      b_1d = tf.reshape(self.W[2*i+1],[-1]) # flatten biases\n",
        "      #print('b_1d : ', b_1d, 'in layer : ',i)\n",
        "\n",
        "      parameters_1d = tf.concat([parameters_1d, w_1d],axis=0) # concat weights\n",
        "      #print('paramters concat weights : ', parameters_1d, np.shape(parameters_1d))\n",
        "      parameters_1d = tf.concat([parameters_1d, b_1d],axis=0) # concat biases\n",
        "      #print('paramters concat biases : ', parameters_1d, np.shape(parameters_1d))\n",
        "\n",
        "    return parameters_1d\n",
        "\n",
        "  def set_weights(self,parameters): # paramters = [.....W_i,b_i,.......]\n",
        "    for i in range(len(layers)-1):\n",
        "      shape_w = tf.shape(self.W[2*i]).numpy() # shape of weight tensor\n",
        "      size_w = tf.size(self.W[2*i]).numpy() # size of weight tensor\n",
        "      #print('shape and size of w : ',shape_w,size_w, 'in layer i : ',i)\n",
        "\n",
        "      shape_b = tf.shape(self.W[2*i+1]).numpy() # shape of bias tensor\n",
        "      size_b = tf.size(self.W[2*i+1]).numpy() # size of bias tensor\n",
        "      #print('shape and size of b : ',shape_b,size_b, 'in layer b : ',i)\n",
        "\n",
        "      pick_w = parameters[0:size_w] # pick the weights\n",
        "      #print('pick_w : ', pick_w)\n",
        "      self.W[2*i].assign(tf.reshape(pick_w,shape_w)) # assign\n",
        "      #print('assigned weights : ', self.W[2*i].assign(tf.reshape(pick_w,shape_w)) )\n",
        "      #print('parameters before deleting weights: ', parameters, np.shape(parameters))\n",
        "      parameters = np.delete(parameters, np.arange(size_w),0) # delete\n",
        "      #print('parameters after deleting weights: ', parameters, np.shape(parameters))\n",
        "\n",
        "      pick_b = parameters[0:size_b] # pick the biases\n",
        "      self.W[2*i+1].assign(tf.reshape(pick_b,shape_b)) # assign\n",
        "      #print('parameters before deleting biases: ', parameters, np.shape(parameters))\n",
        "      parameters = np.delete(parameters, np.arange(size_b),0) # delete\n",
        "      #print('parameters after deleting biases : ', parameters, np.shape(parameters))\n",
        "\n",
        "  ## HERE YOU BUILD YOUR NETWORK\n",
        "  def evaluate(self,x):  # x - inputs of neurons\n",
        "    x = (x-lb)/(ub - lb)  # if x = [100,5] then a = [100,5], #feature scaling\n",
        "    #print('Network input : ',np.shape(x), x)\n",
        "    a = x\n",
        "\n",
        "    for i in range(len(layers)-2):\n",
        "      z = tf.add(tf.matmul(a,self.W[2*i]), self.W[2*i+1])\n",
        "      #print('z : ', np.shape(z))\n",
        "      a = tf.nn.tanh(z)\n",
        "      #print('a : ', np.shape(a))\n",
        "\n",
        "    a =  tf.add(tf.matmul(a,self.W[-2]),self.W[-1]) # For regression, no activation to last layer\n",
        "    #print('a_last layer : ', np.shape(a))\n",
        "    return a  # output of the network; y = [100,1]\n",
        "\n",
        "\n",
        "  def loss_IC(self,x,y): # x = [100,5]   x - network output, y = true values\n",
        "    loss_u_ic = tf.reduce_mean(tf.square(y-self.evaluate(x)))  # y_cap[100,1] - y_true[100,1]\n",
        "\n",
        "    # implementing du_dt IC\n",
        "    g = tf.Variable(x, dtype='float64', trainable=False) # (6000, 5)\n",
        "    x_f = g[:,0:1] # (6000, 1)\n",
        "    y_f = g[:,1:2] # (6000, 1)\n",
        "    t_f = g[:,2:3] # (6000, 1)\n",
        "    dx_f = g[:,3:4] # (6000, 1)\n",
        "    dy_f = g[:,4:5] # (6000, 1)\n",
        "    #----------------------------------------------\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      tape.watch(t_f)\n",
        "      g = tf.stack([x_f[:,0], y_f[:,0], t_f[:,0], dx_f[:,0],dy_f[:,0]],axis=1) # (6000, 5)\n",
        "      z  = self.evaluate(g) # passing PDE points into network#\n",
        "\n",
        "    u_t = tape.gradient(z,t_f)\n",
        "    #print('u_t: ', tf.shape(u_t))\n",
        "\n",
        "    loss_du_ic = tf.reduce_mean(tf.square(y-u_t))  # y_cap[100,1] - y_true[100,1]\n",
        "\n",
        "    loss_ic = loss_u_ic + loss_du_ic\n",
        "\n",
        "    return loss_ic\n",
        "\n",
        "  def loss_BC(self,x,y): # x = [100,5]   x - network output, y = true values\n",
        "    loss_bc = tf.reduce_mean(tf.square(y-self.evaluate(x)))  # y_cap[100,1] - y_true[100,1]\n",
        "    return loss_bc\n",
        "\n",
        "  def excitation(self,x,y,t,source_val): # Excitation source\n",
        "\n",
        "    excitation_center = (scaled_Lx/2, scaled_Ly/2)  # center of excitation source\n",
        "    # XY_s = tf.concat([x[:,0], y[:,0]], 1)\n",
        "\n",
        "    # r = np.array([((xy[0] - excitation_center[0]) ** 2 + (xy[1] - excitation_center[1]) ** 2) ** 0.5 for xy in XY_s])\n",
        "\n",
        "    x_dist = x.numpy().all() - excitation_center[0]\n",
        "    y_dist = y.numpy().all() - excitation_center[0]\n",
        "    r = np.sqrt(x_dist**2 + y_dist**2)\n",
        "    #print('radius : ', r)\n",
        "    excitation_radius = 0.0  # Set the excitation radius - how much does the excitation spread\n",
        "\n",
        "    excitation = source_val\n",
        "    #print('excitation : ', np.shape(excitation))\n",
        "    return excitation if r < excitation_radius else 0.0\n",
        "    #return excitation[r < excitation_radius,:]\n",
        "\n",
        "  def loss_PDE(self, x_to_train_f):\n",
        "    g = tf.Variable(x_to_train_f, dtype='float64', trainable=False)\n",
        "\n",
        "    # Material parameters\n",
        "    ''' E = 120 GPa and density = 7900 kg/m^3, c^2 = E/density'''\n",
        "    original_E = tf.constant(value=180e9, dtype='float64')\n",
        "    original_rho = tf.constant(value=7900, dtype='float64')\n",
        "    E_scaled = original_E/(beta**2)\n",
        "    rho_scaled = original_rho/(alpha**2)\n",
        "    c_scaled = np.sqrt(E_scaled/rho_scaled) # velocity of wave propagating in medium\n",
        "    beta_sqr = beta**2\n",
        "    # Known damage parameters\n",
        "    A = tf.constant(value=0.9, dtype='float64')\n",
        "    # x_0 = y_0  = tf.constant(value=0.12, dtype='float64')\n",
        "    sigma_x = sigma_y = tf.constant(value=0.03, dtype='float64')\n",
        "\n",
        "    x_pde_scaled = g[:,0:1] # (6000, 1)\n",
        "    y_pde_scaled = g[:,1:2] # (6000, 1)\n",
        "    t_pde_scaled = g[:,2:3] # (6000, 1)\n",
        "    x0_scaled = g[:,3:4] # (6000, 1)\n",
        "    y0_scaled = g[:,4:5] # (6000, 1)\n",
        "\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape: #To compute multiple gradients over the same computation, create a gradient tape with persistent=True\n",
        "      tape.watch(x_pde_scaled)\n",
        "      tape.watch(y_pde_scaled)\n",
        "      tape.watch(t_pde_scaled)\n",
        "      tape.watch(x0_scaled)\n",
        "      tape.watch(y0_scaled)\n",
        "\n",
        "      with tf.GradientTape(persistent=True) as doubletape:\n",
        "        doubletape.watch(x_pde_scaled)\n",
        "        doubletape.watch(y_pde_scaled)\n",
        "        doubletape.watch(t_pde_scaled)\n",
        "        doubletape.watch(x0_scaled)\n",
        "        doubletape.watch(y0_scaled)\n",
        "\n",
        "        g = tf.stack([x_pde_scaled[:,0], y_pde_scaled[:,0], t_pde_scaled[:,0], x0_scaled[:,0], y0_scaled[:,0]],axis=1) # (6000, 5)\n",
        "        #print('g: ', np.shape(g))\n",
        "\n",
        "        u_scaled  = self.evaluate(g) # passing PDE points into network#\n",
        "        #print('Network output z : ',tf.shape(z))\n",
        "\n",
        "        # Damage calculation (scaled)\n",
        "        x_part = ((x_pde_scaled - x0_scaled)**2)/(2*sigma_x**2)\n",
        "        y_part = ((y_pde_scaled - y0_scaled)**2)/(2*sigma_y**2)\n",
        "        damage_function_scaled = A*tf.exp(-beta_sqr*(x_part + y_part)) # (6000,1)\n",
        "        #print('damage : ',tf.shape(damage))\n",
        "\n",
        "      u_x_scaled = doubletape.gradient(u_scaled,x_pde_scaled)\n",
        "      #print('u_x : ', tf.shape(u_x))\n",
        "\n",
        "      u_y_scaled = doubletape.gradient(u_scaled,y_pde_scaled)\n",
        "      #print('u_y: ', tf.shape(u_y))\n",
        "\n",
        "      u_t_scaled = doubletape.gradient(u_scaled,t_pde_scaled)\n",
        "      #print('u_t: ', tf.shape(u_t))\n",
        "\n",
        "      damage_x_scaled = doubletape.gradient(damage_function_scaled,x_pde_scaled)\n",
        "      #print('d_x: ', tf.shape(d_x))\n",
        "\n",
        "      damage_y_scaled = doubletape.gradient(damage_function_scaled,y_pde_scaled)\n",
        "      #print('d_y: ', tf.shape(d_y))\n",
        "\n",
        "    u_xx_scaled = tape.gradient(u_x_scaled,x_pde_scaled)\n",
        "    #print('u_xx : ', tf.shape(u_xx))\n",
        "\n",
        "    u_yy_scaled = tape.gradient(u_y_scaled,y_pde_scaled)\n",
        "    #print('u_yy : ', tf.shape(u_yy))\n",
        "\n",
        "    u_tt_scaled = tape.gradient(u_t_scaled, t_pde_scaled)\n",
        "    #print('u_tt : ', tf.shape(u_tt))\n",
        "\n",
        "    del tape # Drop the reference to the tape\n",
        "\n",
        "    # # source location\n",
        "    # x_s = y_s = tf.constant(value=0.5, dtype='float64')\n",
        "    # sigma_s = tf.constant(value=0.001, dtype='float64')\n",
        "    # x_part_s = ((x_pde - x_s)**2)/(2*sigma_s**2)\n",
        "    # y_part_s = ((y_pde - y_s)**2)/(2*sigma_s**2)\n",
        "    # source_location = tf.exp(-(x_part_s + y_part_s)) # (6000,1)\n",
        "\n",
        "    #pde_res_scaled = u_tt_scaled - (c_scaled**2)*(((1-damage_function_scaled)*(u_xx_scaled + u_yy_scaled)) - (damage_x_scaled*u_x_scaled + damage_x_scaled*u_y_scaled)) - (1/rho_scaled)*self.excitation(x_pde_scaled[:,0], y_pde_scaled[:,0], t_pde_scaled[:,0],source_value_arr)\n",
        "    pde_res_scaled = u_tt_scaled - (c_scaled**2)*(((1-damage_function_scaled)*(u_xx_scaled + u_yy_scaled)) - (damage_x_scaled*u_x_scaled + damage_x_scaled*u_y_scaled))\n",
        "\n",
        "    loss_pde_scaled = tf.reduce_mean(tf.square(pde_res_scaled))\n",
        "\n",
        "    return loss_pde_scaled\n",
        "\n",
        "\n",
        "  def loss_source(self, x_src, u_src ):\n",
        "    loss_src = tf.reduce_mean(tf.square(u_src - self.evaluate(x_src)))  # y_cap[100,1] - y_true[100,1]\n",
        "    return loss_src\n",
        "\n",
        "\n",
        "  def loss(self,x_ic,y_ic,x_bc,y_bc, g , x_src, u_src):\n",
        "    loss_u_ic = self.loss_IC(x_ic,y_ic)\n",
        "    loss_u_bc = self.loss_BC(x_bc,y_bc)\n",
        "    loss_pde = self.loss_PDE(g)\n",
        "    loss_source = self.loss_source(x_src, u_src)\n",
        "\n",
        "    loss = loss_u_ic + loss_u_bc + loss_pde + loss_source\n",
        "    #print(loss, loss_u_ic, loss_u_bc, loss_f)\n",
        "\n",
        "    return loss, loss_u_ic, loss_u_bc, loss_pde, loss_source\n",
        "\n",
        "\n",
        "  def optimizerfunc(self,parameters): # parameters = init_params = PINN.get_weights().numpy()\n",
        "    self.set_weights(parameters)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      tape.watch(self.trainable_variables) # dig a little more about this\n",
        "\n",
        "      loss_val, loss_u_ic, loss_u_bc, loss_pde, loss_source = self.loss(X_u_ic_train, u_ic_train, X_u_bc_train, u_bc_train, X_pde_train, X_pde_train_source, source_value_arr)\n",
        "\n",
        "    grads = tape.gradient(loss_val, self.trainable_variables)\n",
        "\n",
        "    del tape\n",
        "\n",
        "    grads_1d = [] # flatten grads\n",
        "\n",
        "    for i in range(len(layers)-1):\n",
        "      grads_w_1d = tf.reshape(grads[2*i],[-1]) #flatten weights\n",
        "      grads_b_1d = tf.reshape(grads[2*i+1],[-1]) #flatten biases\n",
        "\n",
        "      grads_1d = tf.concat([grads_1d, grads_w_1d], 0) #concat grad_weights\n",
        "      grads_1d = tf.concat([grads_1d, grads_b_1d], 0) #concat grad_bias\n",
        "\n",
        "    return loss_val.numpy(), grads_1d.numpy()\n",
        "\n",
        "\n",
        "  def optimizer_callback(self,parameters):\n",
        "\n",
        "      loss_value, loss_u_ic, loss_u_bc, loss_f, loss_source = self.loss(X_u_ic_train, u_ic_train, X_u_bc_train, u_bc_train, X_pde_train, X_pde_train_source, source_value_arr)\n",
        "      #tf.print(loss_value, loss_u_ic, loss_u_bc, loss_f)\n",
        "\n",
        "      print(loss_value.numpy(), loss_u_ic.numpy(), loss_u_bc.numpy(), loss_f.numpy(), loss_source.numpy())\n",
        "\n",
        "  def predict(self, x_test,y_test,t_test,dx_test,dy_test):\n",
        "    X_u_test = np.stack([x_test[:,0], y_test[:,0], t_test[:,0], dx_test[:,0], dy_test[:,0]],axis=1)\n",
        "    print(np.shape(X_u_test))\n",
        "    u_pred = self.evaluate(X_u_test)\n",
        "    #error_vec = np.linalg.norm((u_star-u_pred),2)/np.linalg.norm(u_star,2)\n",
        "\n",
        "    return u_pred\n"
      ],
      "metadata": {
        "id": "eAeKuE0UdlGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def figsize(scale, nplots = 1):\n",
        "    fig_width_pt = 390.0                          # Get this from LaTeX using \\the\\textwidth\n",
        "    inches_per_pt = 1.0/72.27                       # Convert pt to inch\n",
        "    golden_mean = (np.sqrt(5.0)-1.0)/2.0            # Aesthetic ratio (you could change this)\n",
        "    fig_width = fig_width_pt*inches_per_pt*scale    # width in inches\n",
        "    fig_height = nplots*fig_width*golden_mean              # height in inches\n",
        "    fig_size = [fig_width,fig_height]\n",
        "    return fig_size\n",
        "\n",
        "pgf_with_latex = {                      # setup matplotlib to use latex for output\n",
        "    \"pgf.texsystem\": \"pdflatex\",        # change this if using xetex or lautex\n",
        "    \"text.usetex\": True,                # use LaTeX to write all text\n",
        "    \"font.family\": \"serif\",\n",
        "    \"font.serif\": [],                   # blank entries should cause plots to inherit fonts from the document\n",
        "    \"font.sans-serif\": [],\n",
        "    \"font.monospace\": [],\n",
        "    \"axes.labelsize\": 10,               # LaTeX default is 10pt font.\n",
        "    \"font.size\": 10,\n",
        "    \"legend.fontsize\": 8,               # Make the legend/label fonts a little smaller\n",
        "    \"xtick.labelsize\": 8,\n",
        "    \"ytick.labelsize\": 8,\n",
        "    \"figure.figsize\": figsize(1.0),     # default fig size of 0.9 textwidth\n",
        "    \"pgf.preamble\": [\n",
        "        r\"\\usepackage[utf8x]{inputenc}\",    # use utf8 fonts becasue your computer can handle it :)\n",
        "        r\"\\usepackage[T1]{fontenc}\",        # plots will be generated using this preamble\n",
        "        ]\n",
        "    }\n",
        "#mpl.rcParams.update(pgf_with_latex)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# I make my own newfig and savefig functions\n",
        "def newfig(width, nplots = 1):\n",
        "    fig = plt.figure(figsize=figsize(width, nplots))\n",
        "    ax = fig.add_subplot(111)\n",
        "    return fig, ax\n",
        "\n",
        "def savefig(filename, crop = True):\n",
        "    if crop == True:\n",
        "#        plt.savefig('{}.pgf'.format(filename), bbox_inches='tight', pad_inches=0)\n",
        "        plt.savefig('{}.pdf'.format(filename), bbox_inches='tight', pad_inches=0)\n",
        "        plt.savefig('{}.eps'.format(filename), bbox_inches='tight', pad_inches=0)\n",
        "    else:\n",
        "#        plt.savefig('{}.pgf'.format(filename))\n",
        "        plt.savefig('{}.pdf'.format(filename))\n",
        "        plt.savefig('{}.eps'.format(filename))"
      ],
      "metadata": {
        "id": "DVbKQFoS1LiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_u_ic = 10000\n",
        "N_u_bc =  10000 # Total number of data points for 'u'\n",
        "N_pde =  50000 # Total number of PDE collocation points and corresponding data points\n",
        "\n",
        "X_pde_train, X_pde_train_source, u_pde_train, X_u_ic_train, u_ic_train, X_u_bc_train, u_bc_train = trainingdata(N_u_ic, N_u_bc, N_pde)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwx3-PB8UmAe",
        "outputId": "3cc08248-76a3-4354-da4c-7cc3ee80a82e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_b0 :  (22801, 5)\n",
            "t_b0 :  (22801, 1) x_b0 :  (22801, 1) y_b0 :  (22801, 1) dx_b0 :  (22801, 1) dy_b0 :  (22801, 1) u_b0 :  (22801, 1)\n",
            "X_b1 :  (27482, 5)\n",
            "t_b1 :  (27482, 1) x_b1 :  (27482, 1) y_b1 :  (27482, 1) dx_b1 :  (27482, 1) dy_b1 :  (27482, 1) u_b1 :  (27482, 1)\n",
            "X_b2 :  (27482, 5)\n",
            "t_b2 :  (27482, 1) x_b2 :  (27482, 1) y_b2 :  (27482, 1) dx_b2 :  (27482, 1) dy_b2 :  (27482, 1) u_b2 :  (27482, 1)\n",
            "X_b3 :  (27482, 5)\n",
            "t_b3 :  (27482, 1) x_b3 :  (27482, 1) y_b3 :  (27482, 1) dx_b3 :  (27482, 1) dy_b3 :  (27482, 1) u_b3 :  (27482, 1)\n",
            "X_b4 :  (27482, 5)\n",
            "t_b4 :  (27482, 1) x_b4 :  (27482, 1) y_b4 :  (27482, 1) dx_b4 :  (27482, 1) dy_b4 :  (27482, 1) u_b4 :  (27482, 1)\n",
            "all_X_u_ic_train :  (22801, 5)\n",
            "all_u_ic_train :  (22801, 1)\n",
            "X_u_ic_train from random sampling :  (10000, 5)\n",
            "u_ic_train from random sampling :  (10000, 1)\n",
            "all_X_u_bc_train :  (109928, 5)\n",
            "all_u_bc_train :  (109928, 1)\n",
            "X_u_bc_train from random sampling :  (10000, 5)\n",
            "u_bc_train from random sampling :  (10000, 1)\n",
            "t_pde_train :  (50000, 1) x_pde_train :  (50000, 1) y_pde_train :  (50000, 1) dx_pde_train :  (50000, 1) dy_pde_train :  (50000, 1) u_pde_train :  (50000, 1)\n",
            "X_f_train from random sampling :  (50000, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating source vector\n",
        "source_time = X_pde_train_source[:,2][:,None]\n",
        "#print(np.shape(source_time))\n",
        "source_value_arr = np.array([])\n",
        "\n",
        "# specifying source location with Gauss function\n",
        "x_s = y_s = scaled_Lx/2\n",
        "sigma_s = 0.001\n",
        "x_part_s = (X - x_s)**2/(2*sigma_s**2)\n",
        "y_part_s = (Y - y_s)**2/(2*sigma_s**2)\n",
        "#source_location = np.exp(-(x_part_s + y_part_s))\n",
        "\n",
        "# # plot FDM solutions\n",
        "# fig  = plt.figure(figsize = (16, 4))\n",
        "# ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
        "# ax.plot_surface(X, Y, source_location, cmap='coolwarm', edgecolor='none')\n",
        "# ax.set_xlabel('x')\n",
        "# ax.set_ylabel('y')\n",
        "# ax.set_zlabel('u (x, y)')\n",
        "# ax.set_title('Snapshot 1')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "for xyt in X_pde_train_source:\n",
        "  #print(i)\n",
        "\n",
        "  # Excitation source (scaled)\n",
        "  scaled_Vamp = tf.constant(value = 3.000e-09 , dtype='float64')# amplitude of force excitation\n",
        "  scaled_fex = tf.constant(value=4.52 , dtype='float64') # frequency in Hz used in excitation\n",
        "  cycles = tf.constant(value=5 , dtype='float64') # 5 full periods make up the sine burst excitation\n",
        "  scaled_Tex = 1/scaled_fex # time period of the excitation [s]\n",
        "  #print('Tex : ', Tex)\n",
        "\n",
        "  excitation = scaled_Vamp * tf.sin(2*np.pi*scaled_fex*xyt[2]) * tf.sin(np.pi*scaled_fex/cycles*xyt[2])**2\n",
        "  #print(excitation)\n",
        "  source_value_arr = np.append(source_value_arr, excitation)\n",
        "  #print(source_value_arr, np.shape(source_value_arr))\n",
        "\n",
        "source_value_arr = source_value_arr[:,None]\n",
        "#print(np.shape(source_value_arr))"
      ],
      "metadata": {
        "id": "6x49yqkJDYFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(source_value_arr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm72ccJWbVx3",
        "outputId": "fe1fda8e-8372-4e2e-8402-fd2ad930ccee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4149782, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating source vector\n",
        "source_time = X_pde_train[:,2][:,None]\n",
        "#print(np.shape(source_time))\n",
        "source_value_arr = np.array([])\n",
        "\n",
        "for xyt in X_pde_train:\n",
        "  #print(i)\n",
        "  if  xyt[0] ==  scaled_Lx/2  and xyt[1] == scaled_Ly/2 :\n",
        "\n",
        "    # Excitation source (scaled)\n",
        "    scaled_Vamp = tf.constant(value = 3.000e-09 , dtype='float64')# amplitude of force excitation\n",
        "    scaled_fex = tf.constant(value=4.52 , dtype='float64') # frequency in Hz used in excitation\n",
        "    cycles = tf.constant(value=5 , dtype='float64') # 5 full periods make up the sine burst excitation\n",
        "    scaled_Tex = 1/scaled_fex # time period of the excitation [s]\n",
        "    #print('Tex : ', Tex)\n",
        "\n",
        "    excitation = scaled_Vamp * tf.sin(2*np.pi*scaled_fex*xyt[2]) * tf.sin(np.pi*scaled_fex/cycles*xyt[2])**2 #*(t_f < Tex*cycles);\n",
        "    #print(excitation)\n",
        "    source_value_arr = np.append(source_value_arr, excitation)\n",
        "    #print(source_value_arr, np.shape(source_value_arr))\n",
        "\n",
        "  else:\n",
        "    source_value_arr = np.append(source_value_arr, 0.0)\n",
        "\n",
        "\n",
        "\n",
        "source_value_arr = source_value_arr[:,None]\n",
        "#print(np.shape(source_value_arr))"
      ],
      "metadata": {
        "id": "hG6PxmSrN2ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X_pde_train_source[:,2][:,None], source_value_arr, marker='o', alpha=0.2 )\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "mKN6ebeXc1HK",
        "outputId": "22698afe-5509-4132-ce09-871093070bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-5a4ba2d95909>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pde_train_source\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_value_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[1;32m   2860\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2861\u001b[0m         edgecolors=None, plotnonfinite=False, data=None, **kwargs):\n\u001b[0;32m-> 2862\u001b[0;31m     __ret = gca().scatter(\n\u001b[0m\u001b[1;32m   2863\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmarker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2864\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4582\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4584\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must be the same size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4586\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must be the same size"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# network architecture\n",
        "layers = np.array([5,16,16,16,16,1])\n",
        "\n",
        "PINN = ParametricModel_Scaled(layers)\n",
        "\n",
        "init_params = PINN.get_weights().numpy()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "#network = PINN.evaluate(X_f_train)\n",
        "#Train the model with Scipy L-BFGS optimizer\n",
        "results = scipy.optimize.minimize(fun=PINN.optimizerfunc,\n",
        "                                  x0 = init_params,\n",
        "                                  args=(),\n",
        "                                  method='L-BFGS-B',\n",
        "                                  jac=True,\n",
        "                                  callback=PINN.optimizer_callback,\n",
        "                                  options={'disp':None,\n",
        "                                           'maxcor':200,\n",
        "                                           'ftol':1*np.finfo(float).eps,\n",
        "                                           'gtol':5e-98,\n",
        "                                           'maxfun':50000,\n",
        "                                           'maxiter':5000, # Maximum number of iterations.\n",
        "                                           'iprint':-1,\n",
        "                                           'maxls':50})  # Maximum number of line search  (per iteration). Default is 20.\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print('Training time: %.2f' % (elapsed))\n",
        "\n",
        "print(results)\n",
        "\n",
        "PINN.set_weights(results.x)"
      ],
      "metadata": {
        "id": "VcbKfezOdDWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Data\n",
        "snap = np.array([120])\n",
        "x_test = x_star # ((364816, 1)\n",
        "y_test = y_star # (364816, 1)\n",
        "t_test = t_star # (364816, 1)\n",
        "U_test = u_star # (364816, 1)\n",
        "U_test_scaled = u_star_scaled\n",
        "dx_test = dx_star # (364816, 1)\n",
        "dy_test = dy_star # (364816, 1)\n",
        "\n",
        "# Prediction\n",
        "u_pred = PINN.predict(x_test,y_test,t_test,dx_test,dy_test)\n",
        "\n",
        "# u_min = -3.631006664023074e-26\n",
        "# u_max = 3.703959468272658e-26\n",
        "\n",
        "# u_pred_scaled = (u_pred - u_min)/(u_max - u_min)\n",
        "\n",
        "# u_pred = u_pred_scaled\n",
        "\n",
        "error_u = np.linalg.norm(U_test_scaled-u_pred,2)/np.linalg.norm(U_test_scaled,2)\n",
        "print('Error w: %e' % (error_u))\n",
        "\n",
        "u_pred = np.reshape(u_pred,(-1,182))\n",
        "\n",
        "#-------------------Plotting-----------------------------------------\n",
        "snap = snap\n",
        "lb_plot = X_Y_data.min(0)\n",
        "ub_plot = X_Y_data.max(0)\n",
        "nn = 200\n",
        "x_plot = np.linspace(lb_plot[0], ub_plot[0], nn)\n",
        "y_plot = np.linspace(lb_plot[1], ub_plot[1], nn)\n",
        "X_plot, Y_plot = np.meshgrid(x_plot,y_plot)\n",
        "\n",
        "#U_data_plot = griddata(X_Y_data, disp_data[:,snap].flatten(), (X_plot, Y_plot), method='cubic')\n",
        "U_data_plot = griddata(X_Y_data, scaled_disp_data[:,snap].flatten(), (X_plot, Y_plot), method='cubic')\n",
        "U_pred_plot = griddata(X_Y_data, u_pred[:,snap].flatten(), (X_plot, Y_plot), method='cubic')\n",
        "\n",
        "fig, ax = newfig(3.5, 0.4)\n",
        "ax.axis('off')\n",
        "\n",
        "########      Exact     ###########\n",
        "gs = gridspec.GridSpec(1, 3)\n",
        "gs.update(top=0.8, bottom=0.2, left=0.1, right=0.9, wspace=0.5)\n",
        "ax = plt.subplot(gs[:, 0])\n",
        "h = ax.imshow(U_data_plot, interpolation='nearest', cmap='seismic',\n",
        "              extent=[lb_plot[0], ub_plot[0], lb_plot[1], ub_plot[1]],\n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "fig.colorbar(h, cax=cax)\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$y$')\n",
        "ax.set_title('Exact Dynamics', fontsize = 10)\n",
        "\n",
        "\n",
        "########     Learned     ###########\n",
        "ax = plt.subplot(gs[:, 1])\n",
        "h = ax.imshow(U_pred_plot, interpolation='nearest', cmap='seismic',\n",
        "              extent=[lb_plot[0], ub_plot[0], lb_plot[1], ub_plot[1]],\n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "fig.colorbar(h, cax=cax)\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$y$')\n",
        "ax.set_title('Learned Dynamics', fontsize = 10)\n",
        "\n",
        "########     Error     ###########\n",
        "ax = plt.subplot(gs[:, 2])\n",
        "h = ax.imshow(np.abs(U_data_plot - U_pred_plot), interpolation='nearest', cmap='seismic',\n",
        "              extent=[lb_plot[0], ub_plot[0], lb_plot[1], ub_plot[1]],\n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "fig.colorbar(h, cax=cax)\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$y$')\n",
        "ax.set_title('Error', fontsize = 10)\n",
        "\n",
        "plt.savefig('wave2d_scaled.png',dpi = 600)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9vU3yW6x3idv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}